% Header aus der Vorlage
\input{../LaTeX_master/LaTeX_master_HTW}

\bibliography{../Literatur/HTW_Literatur.bib}

% Definition von Titel, Autor usw.
\DTitel{Theoretische Informatik}
\DUntertitel{Vorlesungsskript}
\DAutor{Falk-Jonatan Strube}
\DNotiz{Vorlesung von Dr. Boris Hollas}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\chapter*{Inhalte}
Grundlage: Grundkurs Theoretische Informatik \cite{hollas2015grundkurs}
\begin{itemize}
\item Formale Sprachen
\begin{itemize}
\item Reguläre Sprachen
\begin{itemize}
\item Endliche Automaten
\item Reguläre Ausdrücke
\end{itemize}
\item Nichtreguläre Sprachen
\item Kontextfreie Sprachen
\begin{itemize}
\item Kellerautomaten
\item Grammatiken
\end{itemize}
\end{itemize}
\item Berechenbarkeit
\begin{itemize}
\item Halteproblem
\end{itemize}
\item Komplexitätsklassen
\begin{itemize}
\item $P$
\item $NP$
\item $NP$-vollständige Probleme
\end{itemize}
\end{itemize}

\chapter{Automaten und Formale Sprachen}
%\section{Formale Sprachen}
\paragraph{Def.:} Ein Alphabet ist eine Menge $\Sigma \not = \emptyset$ (Symbole in $\Sigma$ -- müssen nicht einzelne Buchstaben sein, auch Wörter usw. [bspw. „if“ oder „else“ im Alphabet der Programmiersprache C]).

\paragraph{Def.:} Für $w_1, ..., w_n \in \Sigma$ ist $w=w_1...w_n$ ein Wort der Länge $n$.\\
$\Sigma^n$ beschreibt alle Worte mit der Länge genau $n$\\
Das Wort $\varepsilon$ ist das \emph{leere Wort}.\\
Die Menge aller Wörter bezeichnen wir mit $\Sigma^*$ (einschließlich dem leeren Wort).

\subparagraph{Bsp.:} $\Sigma = \{a,b,c\}\quad \rightarrow \Sigma^*=\{\varepsilon, a, b, c, aa, ab,a c, aaa, ...\}$
\paragraph{Def.:} Für Wörter $a,b \in \Sigma^*$ ist $ab$ die Konkatenation dieser Wörter.\\
Für ein Wort $w$ ist $w^n$ die $n$-fache Konkatenation von $w$, wobei $w^0=\varepsilon$.

\subparagraph{Bemerkung:} Für alle $w \in \Sigma^*$ gilt $\varepsilon w = w = w \varepsilon$. $\varepsilon$ ist also das neutrale Element der Konkatenation.

\paragraph{Def.:} Eine \emph{formale Sprache} ist eine Teilmenge von $\sigma^*$.

\paragraph{Def.:} Für Sprachen $A, B$ ist $AB=\{ab \;|\; a \in A, \; b \in B\}$ sowie $A^n=\prod_{i=1}^{n}A$, wobei $A^0=\{\varepsilon\}$.
\subparagraph{Bemerkung:} $\emptyset, \varepsilon, \{\varepsilon\}$ sind unterschiedliche Dinge (leere Menge, leeres Wort, Menge mit leerem Wort).

\subparagraph{Bemerkung:} $\Sigma^*$ lässt sich ebenfalls definieren durch $\Sigma^*=\bigcup_{n\geq 0}\Sigma^n$.\\
Ferner ist $\Sigma^+=\Sigma^*-\{\varepsilon\}$.

\section{Reguläre Sprachen}
\subsection{Deterministische endliche Automaten (DFA)}
\subparagraph{Bsp.:} \parskp
\begin{tikzpicture}[-latex,shorten >= .3em,shorten <= .3em,auto,node distance=6em]
\node[initial, initial text =, state] (z0) {$z_0$};
\node[state] (z1) [right of=z0] {$z_1$};
\node[state] (z2) [right of=z1] {$z_2$};
\node[state, double, double distance = .5mm] (zE) [right of=z2] {$z_E$};
\path (z0) edge [loop above] node{$b,c$} (z0);
\path (z1) edge [loop above] node{$a$} (z1);
\path (zE) edge [loop above] node{$a,b,c$} (zE);
\path (z0) edge [bend left=20] node{$a$} (z1);
\path (z1) edge [bend left=20] node{$c$} (z0);
\path (z1) edge [bend left=20] node{$b$} (z2);
\path (z2) edge [bend left=20] node{$a$} (z1);
\path (z2) edge [bend left = 35] node{$b$} (z0);
\path (z2) edge node{$c$} (zE);
\end{tikzpicture}\\
(Pfeil zeigt auf Startzustand, Endzustand ist doppelt umrandet)\\
Dieser DFA akzeptiert alle Wörter über $\Sigma = \{a,b,c\}$, die $abc$ enthalten.\\
Deterministisch: Es gibt genau ein Folgezustand. Von jedem Knoten aus gibt es genau eine Kante für jedes Zeichen, nicht mehrere und nicht keine.
\subparagraph{Bsp.:} \parskp
\begin{tikzpicture}[-latex,shorten >= .3em,shorten <= .3em,auto,node distance=6em]
\node[initial, initial text = , state, double, double distance = .5mm] (zE){};
\path (z0) edge [loop above] node{$a,b,c$} (zE);
\end{tikzpicture}\\
Dieser DFA erkennt die Sprache $\{a,b,c\}^*$.
\paragraph{Def.:} Ein DFA ist ein Tupel $\mathcal{M}=(Z, \Sigma, \delta, z_0, E)$
\begin{itemize}
\item $Z$: Menge der Zustände
\item $\Sigma$: Eingabealphabet
\item $\delta$: Überführungsfunktion $Z\times \Sigma \rightarrow Z$. Dabei bedeutet $\delta (z,a) = z'$, dass $\mathcal{M}$ im Zustand $z$ für das Zeichen $a$ in den Zustand $z'$ wechselt.
\item $z_0\in Z$: Startzustand
\item $E$: Menge der Endzustände
\end{itemize}
$\delta$: \\
\begin{tikzpicture}[-latex,shorten >= .3em,shorten <= .3em,auto,node distance=6em]
\node[initial, state] (z0) {$z$};
\node[state] (z1) [right of=z0] {$z'$};
\path (z0) edge [bend left=20] node{$a$} (z1);
\end{tikzpicture}

\paragraph{Def.:} Die erweiterte Überführungsfunktion $\hat{\delta}:Z\times \Sigma^*\rightarrow Z$ ist definiert durch\\
$\hat{\delta}(z,w)=\begin{cases}
z & \text{für }w = \varepsilon\\
\hat{\delta}(\delta(z,a),x) & \text{für }w=ax \text{ mit } a\in \Sigma ,x \in \Sigma^*
\end{cases}$\\
Dazu vergleichbarer C-Code:
\begin{lstlisting}[language=C]
int $\hat{\delta}$(int z, char* w){
	if ( strlen(w) == 0 )
		return z;
	else
		return $\hat{\delta}$($\delta$(z, w[0]), w[1]);
\end{lstlisting}
Veranschaulichung:\\
\begin{tikzpicture}[-latex,shorten >= .3em,shorten <= .3em,auto,node distance=6em]
\node[initial, state] (z0) {$z$};
\node[state] (z1) [above right of=z0] {$z'$};
\node[state, white] (z2) [below right of=z0] {};
\node[state] (z3) [right of=z1, node distance=10em] {};
\path (z0) edge node{$w[0]$} (z1);
\path (z0) edge node{} (z2);
\path [dashed] (z1) edge (z3);
\end{tikzpicture}\\
Die erweiterte Überführungsfunktion bestimmt den Zustand nach dem vollständingen Lesen eines Wortes.

\subparagraph{Bsp.:} \parskp
\begin{tikzpicture}[-latex,shorten >= .3em,shorten <= .3em,auto,node distance=6em]
\node[initial, state] (z0) {$z_0$};
\node[state, double, double distance = .5mm] (zE) [right of=z0] {$z_E$};
\path (z0) edge node{$b$} (zE);
\path (z0) [loop above] edge node{$a$} (z0);
\path (zE) [loop above] edge node{$a,b$} (zE);
\end{tikzpicture}
\begin{align*}
\hat{\delta}(z_0, aaba) &= \hat{\delta}(\delta(z_0,a),aba)=\\
\hat{\delta}(z_0, aba) &= \hat{\delta}(\delta(z_0,a),ba)=\\
\hat{\delta}(z_0, ba) &= \hat{\delta}(\delta(z_0,b),a)=\\
\hat{\delta}(z_E, a) &= \hat{\delta}(\delta(z_E,a), \varepsilon)=\\
\hat{\delta}(z_E, \varepsilon) &= z_E
\end{align*}
Die von $\mathcal{M}$ \emph{akzeptierte Sprache} ist $L(\mathcal{M})=\{w\in \Sigma^* \;|\; \hat{\delta}(z_0, w) \in E\}$

\subsection{Nichtdeterministischer endliche Automaten (NFA)}
ABB23\\
NFA, der alles akzeptiert, was $abc$ enthält:\\
ABB24\\
Beispiel: Wort $abaabcab$
\subparagraph{Beispiel:} NFA, der alle Wörter akzeptiert, die auf $001$ enden:\\
ABB25\\
Akzeptierte Worte unter anderem: $01011001$, $001001$\\
Ein Wort wird vom NFA akzeptiert, wenn es einen Weg, ausgehend von einem Startzustand, gibt, mit den ein End-Zustand erreicht wird.\\
Der NFA „weiß“ nicht, welcher Pfad zu durchlaufen ist; diesen muss der Benutzer ermitteln (wie bei einer Straßenkarte).\\
Ein NFA lässt sich formalisieren durch ein Tupel $\mathcal{M}=(Z, \Sigma, \delta, S, E)$
\begin{itemize}
\item $Z$: Zustände
\item $\Sigma$: Eingabealphabet
\item $\delta$: $Z \times \Sigma \to \mathcal{P}(Z)$ Überführungsfunktion (bildet ab in Potenzmenge von $Z$)
\item $S$: Menge der Startzustände
\item $E$: Menge der Endzustände
\end{itemize}
Dabei bedeutet $\delta(z,a)\ni z'$, dass der NEA im Zustand $z$ für die Eingabe $a$ die Möglichkeit besitzt, in den Zustand $z'$ zu wechseln.

\subsection{Umwandlung eines NFA in einen DFA}\parskp
Wir wollen den NFA\\
ABB 27\\
in einen DFA umwandeln. Der Startzustand des DFA besteht aus den Startzuständen des NFA:\\
ABB 28\\
Betrachten die Folgezustände für $a \in \Sigma$:\\
ABB 29\\
nächster Schritt:\\
ABB 30\\
nächster Schritt:\\
ABB 31\\
weitere Schritte:\\
$z_0,b: \{z_0\}$\\
$z_1,b: \{z_2\}$\\
ABB 32\\
$z_0,c: \{z_0\}$\\
$z_1,c: \{\}$\\
ABB 33\\
usw.:\\
ABB 34\\
Wenn ein Zustand des DFA einen Endzustand des NFA enthält, so ist es ein Endzustand.\\
Der auf diese Weise erhaltene DFA kann Zustände enthalten, die sich zu einem Zustand zusammen fassen lassen. Mit dem Algorithmus Minimalautomat lässt sich ein DFA konstruieren, der minimal bezüglich der Anzahl seiner Zustände ist. Der Minimalautomat ist eindeutig, d.h. Minimalautomaten unterscheiden sich höchstens in der Benennung der Zustände.

\subsection{Reguläre Ausdrücke}
\paragraph{Def.:} Sei $\Sigma$ ein Alphabet. Ein \emph{regulärer Ausdruck} $E$ sowie die durch $E$ \emph{erzeugte Sprache $L(E)$} sind induktiv definiert:
\begin{itemize}
\item $\emptyset$ ist ein regulärer Ausdruck und $L(\emptyset)=\emptyset$.\\
Bsp.: \\
ABB35
\item Für $a \in \Sigma \cup \{\varepsilon\}$ ist $a$ ein regulärer Ausdruck und $L(a)=\{a\}$.
\item Für reguläre Ausdrücke $E_1, E_2$ sind $(E_1 | E_2), \; (E_1 E_2), \; (E_1^*)$ reguläre Ausdrücke (hier: $|=$„oder“) und $L(E_1|E_2)=L(E_1)\cup L(E_2), \; L(E_1E_2)=L(E_1)L(E_2), \; L(E_1^*)=L(E_1)^*$ die davon erzeugten Sprachen:\\
\begin{tabular}{l | r l}
Ausdruck & Sprache &\\
\hline
$E_1 | E_2$ & $L(E_1|E_2)$&$=L(E_1)\cup L(E_2)$\\
$E_1 E_2$ & $L(E_1E_2)$&$=L(E_1)L(E_2)$\\
$E_1^*$ & $L(E_1^*)$&$=L(E_1)^*$\\
\end{tabular}\medskip\\
Hinweis: $E^+=E E^*$, $E?=\varepsilon|E$\\
$\boxed{\text{Wenn }E_1, E_2 \text{ regulär, dann auch } (E_1 | E_2), \; (E_1 E_2), \; (E_1^*) \text{ regulär}}$
\end{itemize}
\subparagraph{Bsp.:} 
\begin{itemize}
\item $L( (0|1)^*) =(L(0|1))^*=(L(0)\cup L(1))^*=(\{0\}\cup\{1\})^*=\{0,1\}^*$
\item Regulärer Ausdruck über $\Sigma=\{a,b,c\}$, der die gleiche Sprache erzeugt wie der DFA aus dem letzten Automaten-Beispiel:\\
$L((a|b|c)^*abc(a|b|c)^*)=\{a,b,c\}^*\{abc\}\{a,b,c\}^*$
\end{itemize}

\paragraph{Satz:} Reguläre Ausdrücke erzeugen genau die regulären Sprachen.
\subparagraph{Skizze:} Umwandlung eines regulären Ausdrucks in einen endlichen Automaten.
\begin{itemize}
\item $\emptyset$: ABB 40
\item $a \in \Sigma$: ABB 41 1.\\
$\varepsilon$: ABB 41 2.
\item Seien $E_1, E_2$ reguläre Ausdrücke und $\mathcal{M}_1, \mathcal{M}_2$ DFAs mit $L(E_1)=L(\mathcal{M}_1), L(E_2)=L(\mathcal{M}_2)$.
\begin{itemize}
\item $E_1 | E_2$: $\mathcal{M}_1, \mathcal{M}_2$ sind zusammen ein NFA, der $L(\mathcal{M}_1) \vee L(\mathcal{M}_2)$ erkennt.\\
Bsp.: $E_1=a, E_2=b$\\
ABB 42
\item $E_1 E_2$: $\mathcal{M}_1, \mathcal{M}_2$ müssen hintereinander geschaltet werden, wobei ggf. neue Kanten eingefügt werden müssen. Dazu betrachtet man die Kante nach der neuen Verbindung und erzeugt dem entsprechend die Übergangskanten.\\
ABB 43
\item $E_1^*$: Es müssen Kanten zurück zum Startzustand eingefügt werden\\
ABB 44
\end{itemize}
\end{itemize}
Der Beweis für die umgekehrte Richtung (DFA $\to$ reg. Ausdruck) ist schwierig.
\subparagraph{Bsp.:}
\begin{itemize}
\item $E=0(0|1)^*$\\
ABB 45
\item ABB 46\\
Beobachtungen:
\begin{itemize}
\item um zum Endzustand zu kommen, braucht man eine $1$.
\item vor der $1$ kann $\varepsilon$ stehen, oder beliebig viele $0$en der $1$en.
\end{itemize}
$\Rightarrow E=(0|1)^* 1$
\end{itemize}

\subsection{Das Pumping-Lemma}
Wenn ein DFA ein Wort akzeptiert, das mindestens so lang ist wie die Anzahl seiner Zustände, dann muss er einen Zustand zweimal durchlaufen (Schubfachprinzip). Daraus folgt, dass der DFA dabei eine Schleife durchläuft.
\subparagraph{Bsp.:}\parskp
ABB 47\\
Für $x=abcdecfg$ durchläuft der Automat eine Schleife: $x=ab\,\boxed{cde}\,cfg$. Daher akzeptiert der DFA auch alle Wärter $ab(cde)^k cfg$ für $k \geq 0$.
\paragraph{Satz:} (Pumping Lemma)\\
Für jede reguläre Sprache $L$ gibt es ein $n>0$ ($n$: Anzahl Zustände des Minimalautomaten), so dass es für alle Wörter $x \in L$ mit $|x|\geq n$ eine Zerlegung $x=uvw$ gibt (in vorherigem Bsp.: $u=ab, \; v=cde \; w =cfg$), so dass gilt:
\begin{enumerate}
\item $|v| \geq 1$
\item $|uv|\leq n$ \quad ($u, w$ können auch $\varepsilon$ sein)
\item $uv^kw \in L$ für alle $k\geq 0$.
\end{enumerate}
Ohne Einschränkung ist $n$ die Anzahl Zustände des Minimalautomaten.\\
$\Rightarrow \forall \text{ regulären Sprachen }L\quad \exists \;n>0 \quad\forall \; x \in L, \; |x| \geq n \quad \exists \;u,v,w \text{ mit }x=uvw \text{ und }|v| \geq 1,|uv|\geq n \quad \forall \; k\geq 0 \; uv^kw\in L$.\\
Das Pumping-Lemma lässt sich nutzen, um zu zeigen, dass eine Sprache nicht regulär ist.
\subparagraph{Bsp.:} Wir zeigen, dass $L=\{a^nb^n|n\in \mathbb{N}\}$ nicht regulär ist.\\
Problemstellung: Der Automat kann sich das $n$ nicht „merken“, um nach $n$ $a$s wieder $n$ $b$s zu erzeugen. \\
Beweis (Widerspruch):
\begin{itemize}
\item Angenommen, $L$ sei regulär.
\item Nach Pumping-Lemma gibt es dann ein $n>0$, so dass sich alle $x\in L$ mit $|x|\geq n$ gemäß Pumping-Lemma zerlegen lassen.
\item Sei $x=a^nb^n$. 
\item Angenommen $v$ enthalte ein $b$, dann wäre $|uv| > n$. \\
Aus $|uv|\leq n$ folgt aber, dass $v$ kein $b$ enthält. aus $|v|\geq 1$ folgt, dass $v$ mindestens ein $a$ enthält.\\
ABB 48
\item Das Wort $uw$ enthält daher weniger $a$s als $b$s und kann somit nicht in $L$ enthalten sein (denn $w$ enthält $b^n$, da $v$ mindestens ein $a$ enthält, ist durch $uw$ mindestens ein $a$ „verloren gegangen“: $uw=a^{n-|v|}b^n$) und ist deshalb nicht in $L$ enthälten, Widerspruch\,\lightning \; \#
\end{itemize}
Vorgehen:
\begin{itemize}
\item ist regulär
\item Def. Pumping Lemma
\item $x$ finden (gilt für alle $x$, also ein günstiges $x$ aussuchen, mit dem sich Beweis führen lässt)
\item durch 1.) und/oder 2.) einschränken
\item durch 3.) zum Widerspruch führen
\end{itemize}

\subparagraph{Bsp.:} Wir zeigen, dass $L=\{zz|z\in \{a,b\}^*\}$ nicht regulär ist.\\
Intuitiver Hinweis: Kann nicht regulär sein, da sich der Automat nicht merken kann, wie viele $a$s und $b$s im ersten $z$ gelesen wurden, um dann das gleiche im zweiten $z$ zu fabrizieren.\\
Beweis: 
\begin{itemize}
\item Angenommen, $L$ ist regulär.
\item Nach Pumping-Lemma gibt es ein $n>0$, so dass sich alle $x \in L$ mit $|x|\geq n$ zerlegen lassen gemäß Pumping-Lemma.
\item Sei $x=a^nb a^nb$.
\item Wegen $|uv|\leq n$ und $|v|\geq 1$ besteht $v$ aus mindestens einem $a$.\\
ABB 61
\item Dann enthält $uw=a^{n-|v|}ba^nb$ (für $k=0$) weniger $a$s in der vorderen Hälfte als in der hinteren Hälfte. Da sich $uw$ deshalb nicht in die Form $zz$ mit $z\in \{a,b\}^*$ brigen lässt, ist $uw \not \in L$, Widerspruch!
\end{itemize}

\paragraph{Satz:} Seien $L$ regulär und $n$ die Anzahl Zustände des Minimalautomaten zu $L$. Dann gilt $|L| = \infty$ genau dann, wenn es ein $x \in L$ gibt mit $n\leq  |x| < 2n$.\\
Beweis:\\
($\Leftarrow$):\\
Gemäß Pumping Lemma gibt es eine Zerlegung $x=uvw$ mit $|v| \geq 1$ und $uv^kw\in L$ für alle $k \in \mathbb{N}_0$ ($\mathbb{N}$ ist unendlich).\\
Daraus folg $|L|=\infty$.\\
($\Rightarrow$):\\
Da es nur endlich viele Wörter $x$ mit $|x|<n$ gibt, gibt es ein $x\in L$ mit $|x|\geq n$.\\
Sei daher $x\in L$ mit $|x| \geq n$ und $|x|$ minimal.\\
Gemäß PL lässt sich $x$ zerlegen in $x=uvw$.\\
Da $uw \in L$ und $|x|$ minimal ist, gilt $|uw|<n$.\\
Wegen $|x|\geq \underbrace{|uv|}_{<n \text{ gemäß PL}}+\underbrace{|uw|}_{<n \text{ Satz zuvor}}<n+n=2n$ folgt die Behauptung $n \leq |x| \leq 2n$. \\
Regulärer Ausdruck: Generator\\
Automat: Validator
\section{Kontextfreie Sprachen}

\subsection{Kellerautomaten (PDA)}
Ein Kellerautomat (Pushdown Automaton, PDA) besitzt gegenüber einem NFA zwei zusätzliche Eigenschaften:
\begin{itemize}
\item Es gibt $\varepsilon$-Übergänge.
\item Er besitzt einen Stack, auf dem Zeichen abgelegt oder von dem Zeichen gelesen werden können.
\end{itemize}
Zur graphischen Darstellung von PDAs verwenden wir eine erweiterte Automatennotation:\\
ABB62\\
Unten auf dem Stack liegt das Symbol \#. Dies ist das einzige Symbol, das sich zu Beginn einer Rechnung auf dem Stack befindet.
\subparagraph{Bsp.:} PDA, der $\{a^nb^n|n\in \mathbb{N}\}$ akzeptiert.\\
ABB63\\
Wir erlauben nun, dass der PDA in einem Schritt auch mehrere Zeichen auf den Stack schreibt. Dazu erweitern wir die graphische Notation wie folgt:\\
ABB 68
\subparagraph{Def.:} Ein PDA ist ein Tupel $M=(Z,\Sigma, \Gamma, \delta, z_0, \#, E)$
\begin{itemize}
\item $Z$: Zustände
\item $\Sigma$: Eingabealphabet
\item $\Gamma$: Stackalphabet
\item $\delta$: $Z\times \Sigma_\varepsilon \times \Gamma_\varepsilon \to \mathcal{P}(Z\times \Gamma_\varepsilon)$, wobei $\Sigma_\varepsilon=\Sigma \cup \{\varepsilon\}$, $\Gamma_\varepsilon = \Gamma \cup \{\varepsilon\}$
\item $z_0\in Z$: Startzustand
\item $\# \in \Gamma$: Unterstes Stackzeichen
\item $E \in Z$: Endzustände
\end{itemize}
ABB 69\\
$a \in \Sigma \cup \{\varepsilon\}$\\
$\gamma \in \Gamma \cup \{\varepsilon\}$\\
$\gamma' \in \Gamma \cup \{\varepsilon\}$

\paragraph{Def.:} Die von einem PDA $M$ akzeptierte Sprache $L(M)$ ist die Menge aller $x \in \Sigma^*$, für die gilt: Der PDA $M$ kann, ausgehend vom Startzustand und dem initialen Stackzustand $\#$, durch das Lesen des Wortes $x$ einen Endzustand erreichen.

\subsection{Kontextfreie Grammatiken}
Eine kontextfreie Grammatik beschreibt, wie durch das Ersetzen von variablen Wörter der Sprache erzeugt werden können. Jede Ersetzungsregel hat die Form „linke Seite $\to$ rechte Seite“ (linke Seite der Regel kann ersetzt werden durch die rechte Seite), wobei „linke Seite“ eine Variable ist. 

Beginnend mit dem Startsymbol werden solange Ersetzungsregeln angewendet, bis alle Variablen durch Terminalsymbole (Elemente aus $\Sigma$) ersetzt wurden.

\subparagraph{Bsp.:}
\begin{itemize}
\item Satz $\to$ NP VP\footnote{Nominalphase, Verbalphase}
\item NP $\to$ Artikel Nomen
\item Artikel $\to$ \tgreen{die}
\item Nomen $\to$ \tgreen{Katze}
\item Nomen $\to$ \tgreen{Maus}
\item VP $\to$ Verb NP
\item Verb $\to$ \tgreen{jagt}
\end{itemize}
Satz $\Rightarrow$ NP VP $\Rightarrow$ Artikel Nomen VP $\Rightarrow$ Artikel Nomen Verb NP $\Rightarrow$ Artikel Nomen Verb Artikel Nomen $\Rightarrow$ … $\Rightarrow$ die Katze jagt die Maus\\
Syntax dazu:\\
ABB 71

\paragraph{Def.:} Eine kontextfreie Grammatik ist ein Tupel $\sigma = (V, \Sigma, P, S)$
\begin{itemize}
\item $V$: Endliche Menge der Variablen oder Nonterminalzeichen
\item $\Sigma$: Alphabet oder Terminalzeichen $V\cap \Sigma = \emptyset$
\item $P$: Regeln oder Produktionen der Form $u \to v$ mit $u \in V$ und $v \in (V\cup \Sigma)^*$
\item $S \in V$
\end{itemize}

Für $x,y\in (V \cup \Sigma)^*$ schreiben wir $x \Rightarrow y$, wenn sich durch das Ersetzen einer Variablen in $x$ die Satzform $y$ erzeugen lässt.
\subparagraph{Bsp.:} die Nomen Verb $\Rightarrow$ die Katze Verb\medskip\\
Die reflexive und transitive Hülle der Relation $\Rightarrow$ bezeichnen wir mit $\Rightarrow^*$. Umgangssprachlich: durch $\Rightarrow^*$ werden nicht alle $\Rightarrow$-Umformungen dargestellt, sondern teils übersprungen.
\subparagraph{Bsp.:} \parskp
Satz $\Rightarrow^*$ die Katze VP, \\
Satz $\Rightarrow^*$ die Katze jagt die Maus.
\paragraph{Def.:} Die von einer Grammatik erzeugte Sprache ist $L(G)=\{w\in \Sigma^* | S \Rightarrow^* w\}$.
\paragraph{Abkürzende Notation:} \parskp
$S\to \varepsilon | 0S1$ für $S\to \varepsilon, \; S \to 0S1$

\subsubsection{Konstruktionsprinzipien für kontextfreie Grammatiken}
Regeln der Form $X\to aXb$ führen zu:\\
ABB 83\\
Dies lässt sich für balancierte Strukturen nutzen.\\
Mit der Regel $X\to XX$ wächst der Syntaxbaum in die Breite.\\
ABB 84\\
Beispiel, die beide Prinzipien anwendet: $S\to [S] | SS | \varepsilon$\\
ABB 85\\
Damit lässt sich bspw. auch folgendes als Grammatik darstellen: $(3*(4+5)-1)*2+1$.
\subparagraph{Bsp.:} Grammatik für arithmetische Ausdrücke.
\begin{itemize}
\item Zahlen: Lassen sich darstellen durch die Grammatik mit den Regeln:\\
$S_N\to 0S_N\,|\,\dots \,|\, 0S_N \,|\, 0 \,|\, ... \,|\, 9$\\
Beispiel für $123$: \\
ABB 86
\item Zeichen: Diese Grammatik verwenden wir, um Ausdrücke darzustellen mit folgender Grammatik:\\
$S_E\to S_E+S_E \,|\, S_E - S_E \,|\, S_E*S_E \,|\, S_E/S_E \,|\, (S_E) \,|\, S_N$\\
Damit lassen sich Ausdrücke erstellen, z.B.:\\
ABB 87\\
$(1+2)*3*4$
\end{itemize}

\subsection{Kellerautomaten und kontextfreie Sprachen}
\paragraph{Satz:} Kellerautomaten akzeptieren genau die kontextfreien Sprachen.\\
Beweis: Wir zeigen nur: Für jede kontextfreie Grammatik $G$ gibt es einen PDA $\cM$ mit $L(G)=L(\cM)$.\\
Skizze: \\
Wir konstruieren einen PDA mit drei Zuständen:\\
ABB 88\\
Im Startzustand wird das Startsymbol $S$ der Grammatik auf den Stack abgelegt und der PDA wechselt in Zustand $Z$.\\
Wir unterscheiden 3 Fälle: Das oberste Stackzeichen ist…
\begin{itemize}
\item eine Variable $A$.\\
Wenn es eine Regel $A\to \gamma$ der Grammatik gibt, dann kann der PDA $A$ vom Stack entfernen und $\gamma$ auf den Stack schreiben.
\item ein Symbol $a\in \Sigma$.\\
Wenn das nächste Zeichen der Eingabe mit $a$ übereinstimmt, wird $a$ vom Stack entfernt.
\item das Zeichen $\#$.\\
Dann geht der PDA in den Endzustand über.
\end{itemize}
Richtung PDA$\to$kontextfreie Grammatik: Ohne Beweis.
\subparagraph{Bsp.:} Wir betrachten die Sprache $L=\{a^nb^n|n\geq 0\}$, die erzeugt wird von der Grammatik mit den Regeln:\\
$S\to aSb \,|\, \varepsilon$\\
Aus obiger Konstruktion erhalten wir folgenden PDA:\\
ABB 89\\
Verhalten für die Eingabe $aabb$:\\
ABB 90

\subsubsection{Der CYK-Algorithmus}
\paragraph{Def.:} Eine Grammatik $G=(V,\Sigma, P, S)$ liegt in \emph{Chomsky-Normalform} (CNF), wenn alle Regeln die Form $A\to BC$ oder $A\to a$ für $A,B,C \in V$, $a \in \Sigma$ haben.\\
Jede kontextfreie Grammatik $G$ mit $\varepsilon \not \in L(G)$ kann in CNF umgeformt werden.
\subparagraph{Bsp.:} Wir formen die Grammatik  mit den Regeln $S\to SS \,|\, (S) \,|\, ()$ in CNF um.
\begin{enumerate}[label=\arabic*.]
\item Schritt:\\
Terminalsymbole ersetzen durch neue Variablen: \\
$S\to SS \,|\, LSR \,|\, LR$\\
$L\to ($, $R \to )$
\item Schritt:\\
Mehrfache Variablen auf der rechten Seite ersetzen:\\
$S\to SS \,|\, LA \,|\, LR$\\
$A \to SR$\\
$L\to ($\\
$R\to )$
\end{enumerate}
Der Ableitungsbaum eines Wortes aus einer Grammatik in CNF ist -- bis auf die unterste Ebene -- ein binärer Wurzelbaum.\\
ABB 91\\
Wenn ein Wort $x=x_1x_2\dots x_n$ aus $S$ ableitbar ist ($S\Rightarrow^*x$), dann gibt es ein $ k $ und $ A,B $, sodass $ S\Rightarrow AB $ und $ A \Rightarrow* x_1\dots x_k , \; B\Rightarrow^*x_{k+1}...x_n $\\
ABB 92 / 96\\
Der CYK-Algorithmus entscheidet das Wortproblem, indem eine Tabelle konstruiert wird. Der Eintrag $T_{ij}$ ist die Menge der Variablen $X$ mit $X\Rightarrow^* x_i \dots x_j$:\\
ABB 97\\
Für $i<j$ wird geprüft, ob sich $x_i\dots x_j$ zerlegen lässt in $x_i\dots x_k,\; x_{k+1} \dots x_j$, so dass gilt:
\begin{enumerate} [label=(\roman*)]
\item es gibt eine Regel $X\to AB$
\item $A\Rightarrow^* x_i \dots x_k$, $B\Rightarrow^* x_{k+1}\dots x_j$.
\end{enumerate} 
Die Menge $T_{ij}$ enthält alle Variablen $x$ mit dieser Eigenschaft. Da (ii) nach Definition von $T_{ij}$ äquivalent ist zu $A \in T_{ik}, \; B\in T_{k+1\,j}$, erhalten wir $T_{(ii)}=\{x| \text{ es gibt eine Regel }X\to x_i\}$.\\
$T_{ij}=\bigcup_{i\leq k<j} \{x | \text{ es gibt eine Regel }X\to AB \wedge A \in T_{ik} \wedge B\in T_{k+1\, j}\}$\\
Wenn die Menge $T_{ij}$ in aufsteigender Reihenfolge von $j-i$ berechnet werden, dann können die Einträge der Tabelle aus bereits berechneten Einträgen bestimmt werden (dynamisches Programmieren). Für den Eintrag $T_{ij}$ müssen dabei alle Kombinationen geprüft werden, die den Zerlegungen $x_i \dots x_j=x_i\dots x_k x_{k+1} \dots x_j$ für $i\leq k < j$ entsprechen. Das Wort $x$ liegt genau dann in der Sprache, wenn $S \in T_{1n}$.\\
ABB 99\\
$T_{26}=\bigcup_{2\leq k<5}\{x| x\to AB \wedge \underbrace{A \Rightarrow^* x_2\dots x_k}_{\Leftrightarrow A \in T_{2k}}, \; \underbrace{B\Rightarrow^* x_{k+1}\dots x_6}_{\Leftrightarrow B \in T_{k+1\, 6}}\}$
\begin{itemize}
\item $k=2$: $A\in T_{22},\; B \in T_{36}$ (lila)
\item $k=3$: $A \in T_{23}, \; B \in T_{46}$ (grün)
\item $k=4$: $A\in T_{24}, \; B \in T_{56}$ (braun)
\item $k=5$: $A \in T_{25}, \; B \in T_{66}$ (hellgrün)
\end{itemize}
\subparagraph{Bsp.:} Wir prüfen $(()()) \in L(G')$ für die Grammatik $G'$ in CNF, die die Sprache der korrekten Klammerung erzeugt.\\
ABB100 \\
Die Laufzeit des CYK-Algorithmus ergibt sich aus\\
(Größe der Tabelle)$\cdot$(Aufwand pro Tabelleneintrag)$=O(n^2) \cdot O(n)=O(n^3)$.
\begin{lstlisting}[language=C]
$T_{ij}:= \emptyset$
for (k=i; k<j; k++) {
	if (Regel $X\to AB$ $\wedge$ $A \in T_{ik}$ $\wedge$ $B \in T_{k+1\,j}$)
		$T_{ij}$ += {x}
}
\end{lstlisting}

\subsection{Mehrdeutigkeit}
Grammatik für arithmetische Ausdrücke:\\
$E\to E+E \,|\, E - E \,|\, E*E \,|\, E/E \,|\, x \,|\, y \,|\, z$\\
Daraus lässt sich abbilden:\\
ABB 111\\
Diese Grammatik ist auch mehrdeutig, wenn man sich auf nur einen Operator beschränkt:\\
ABB 112\\
Da $-$ links-assoziativ ist, stellt nur der linke Ableitungsbaum die korrekte Interpretation des Ausdrucks $x-y-z$ dar.\\
$\Rightarrow$ Ableitungsbäume dürfen nicht verdreht werden!\\
Eine Grammatik $G$ heißt \emph{eindeutig}, wenn es für alle Wörter $w \in L(G)$ \emph{genau einen} Ableitungsbaum gibt.\\
Um eine eindeutige Grammatik zu erhalten, müssen zwei Probleme gelöst werden:
\begin{enumerate}
\item Die Priorität der Operatoren
\item Die Assoziativität der Operatoren
\end{enumerate}
… müssen beachtet werden.\\
Lösung für:
\begin{enumerate}
\item Die Grammatik muss so konstruiert werden, dass die Strichoperatoren nur auf der obersten Ebene, die Punktoperatoren nur auf der untersten Ebene erzeugt werden können.\\
$E\to E+E \,|\, E-E \,|\, F \\
F \to F*F \,|\, F/F \,|\, x \,|\, y \,|\, z$
\item Die Grammatik muss so beschaffen sein, dass der Ableitungsbaum, gemäß der Richtung der Assoziativität, bei einem links-assoziativen Operator nur nach links wachsen kann.\\
Basierend auf der Lösung für 1.) (mit $T$: Term, $F$: Faktor):\\
$E\to E+T \,|\, E-T \,|\, T \\
T \to T*F \,|\, T/F \,|\, F\\
F \to x \,|\, y \,|\, z$
\end{enumerate}
Diese Grammatik ist eindeutig.\\
Der Ableitungsbaum für $x*y+z$ ist:\\
ABB 113\\
Der Ableitungsbaum für $x-y-z$ ist:\\
ABB 114

\subsection{Syntaxanalyse}

\begin{lstlisting}[language=C]
if ( x<0 || y<0 ) {
	...
} else if
	...
\end{lstlisting}
$\downarrow$\\
Lexer\\
ABB 115\\
$\downarrow$\\
Parser\\
$\downarrow$\\
Syntaxbaum
\paragraph{Ziel:} Aus einem Wort einer kontextfreien Sprache soll ein soll ein Syntaxbaum erzeugt werden. Der CYK-Algorithmus ist dafür geeignet, besitzt jedoch eine Laufzeit in $O(n^3)$. Für deterministische kontextfreie Sprachen lässt sich das Wortproblem in Zeit $O(n)$ entscheiden.\\
Wir erlauben dem Parser, die nächsten $k$ Zeichen der Eingabe (\emph{lookahead}) zu sehen, um abhängig davon Entscheidungen zu treffen.

\subsubsection{Top-Down-Parser}
Ein Top-Down-Parser baut den Syntaxbaum von oben nach unten auf. Ein \emph{Recursive Descent Parser} ist ein Top-Down-Parser, der die Regeln der kontextfreien Grammatik als rekursive Funktionen implementiert.

Beispiel für Sprache $a^nb^n$:
\begin{lstlisting}[language=java]
public class Parser {
	String input;
	int pos;
	
	boolean parse (String inptu0) {
		input = input0 + "#";
		pos = 0;
		return S() && match('#');
	}
	
	boolean S() {	
		if( next() == 'a' )
			return match('a') && S() && match('b');	// entspricht S$\to$aSb
		else return true;	// entsrpicht S$\to$$\varepsilon$
	}
	
	char next() {
		return input.charAt(pos);
	}
	
	boolean match(char c) {	// entspricht Schleife im Kellerautomat: a,a/$\varepsilon$ und b,b/$\varepsilon$ bzw. S,$\varepsilon$/aSb
		if( next() == c ){
			pos++;
			return true;
		}
		else return false;
	}
	
	public static void main(String[] args){
		Parser p = new Parser();
		System.out.println(p.parse("aabb"));	// true
		System.out.println(p.parse("aaabb"));	// false
		System.out.println(p.parse("aabbb"));	// false
	}
}
\end{lstlisting}
Ablauf des Programms für \emph{aabb}:\\
ABB 116\\
Achtung: nicht alle Grammatiken lassen sich mit einem rekursiven Abstiegsparser darstellen. Bsp.: $E\to E+T$:
\begin{lstlisting}[language=java]
boolean E() {
	return E() && match('+') && T();	// Endlosschleife durch Selbstaufruf
}	
\end{lstlisting}
Aus der bereits behandelten Grammatik für arithmetische Ausdrücke kann kein Recursive Descent Parser erzeugt werden, weil die Grammatik linksrekursiv ist. Mögliche Abhilfe: Beseitigung der linksrekursion durch Umbau der Grammtik.\\
Ansatz:\\
$E \to T \tred{(}+E \tred{)} \,|\, T \tred{(}-E\tred{)} \,|\, T$\\
$T \to F \tred{(}*T\tred{)} \,|\, F \tred{(}/T \tred{)} \,|\, F \tred{(\varepsilon)}$\\
$\Rightarrow$\\
$E \to T E'\\
E' \to \varepsilon \,|\, + E \,|\, -E\\
T \to FT'\\
T' \to \varepsilon \,|\, *T \,|\, /T\\
F \to x\,|\,y\,|\,z$\\
Diese Grammatik erzeugt die gleiche Sprache wie die vorherige Grammatik und ist nicht linksrekursiv. Aber die Ableitungsbäume wachsen nach rechts, d.h. alle Operatoren sind rechts-assoziativ.\\
ABB 117\\
Das Problem lässt sich für Recursive Descent Parser nicht befriedigend lösen, Man kann links-assoziative Operatoren durch Schleifen verarbeiten. Dazu: EBNF (Extended Backus-Naur-Form).\\
Obige Grammatik in EBNF:\\
$E\to T \{ \{+|-\} T \}$\\
$T \to F \{ (*|/)F\}\\
F \to x\,|\,y\,|\,z$\\
Dabei bedeutet $\{ +T \}$, dass beliebig viele $+T$ folgen können. $\{...\}$ entspricht $(...)^*$.\\
Problem: Aus dieser Grammatik geht die Assoziativität der Operatoren nicht hervor. Diese muss festgelegt werden. Links-assoziative Operatoren können mit einer Schleife verarbeitet werden:
\begin{lstlisting}[language=C]
E() {
 T();
 while (next() == '+') {	// while-Schleife entspricht $\{+T\}$
 	match ('+');
 	T();
 }
}
\end{lstlisting}
(SimpleInfixCalc.java)


\subsubsection{Bottom-Up-Parser}
Ein \emph{Bottom-Up-Parser} baut einen Ableitungsbaum von unten nach oben auf und kontrolliert dabei die Rechtsableitung der Eingabe. Bottom-Up-Parser lassen sich effizient durch LR-Parser implementieren. Ein LR-Parser liest die Eingabe von links nach rechts und erzeugt den Ableitungsbaum der Rechtsableitung. Ein LR-Parser führt in jedem Schritt eine von vier möglichen Aktionen aus:
\begin{itemize}
\item \textbf{Shift}: Das nächste Zeichen der Eingabe wird auf den Stack geschoben.
\item \textbf{Reduce}: Ein oder mehrere Symbole von der Spitze des Stack entsprechen der rechten Seite $A \to \gamma$ einer Regel und werden durch $A$ ersetzt.
\item \textbf{Accept}: Die Eingabe wurde verarbeitet, der Stock enthält nur das Startsymbol.
\item \textbf{Error}: Ein Syntaxfehler wird gemeldet.
\end{itemize}
\subparagraph{Bsp.:} Wir betrachten die Grammatik für arithmetische Ausdrücke:\\
$E \to E + T \,|\, E-T \,|\, T\\
T \to T*F \,|\, T/F \,|\, F\\
F \to x \,|\, y \,|\, z$\\
Für die Eingabe $x+y*z$ führt ein LR-Parser folgende Schritte aus:\\
\begin{tabular}{l r c}
Stack & restl. Eingabe &Aktion\\
\hline
& $x+y*z$ & shift\\
$x$ & $+y*z$ & reduce\\
$F$ & $+y*z$ & reduce\\
$T$ & $+y*z$ & reduce\\
$E$ & $+y*z$ & shift \\
$E+$ & $y*z$ & shift \\
$E+y$ & $*z$ & reduce\\
$E+F$ & $*z$ & reduce\\
$E+T$ & $*z$ & shift\footnotemark\\
$E+T*$ & $z$ & shift \\
$E+T*z$ & & reduce \\
$E+T*F$ & & reduce \\
$E+T$ & & reduce \\
$E$ & & accept
\end{tabular}
\footnotetext{hier würde reduce stecken bleiben, weil es keine Regel für $E+E$ geben würde. Der Parser „weiß“ das aus Ableitungstabellen.}\\
Der vom Parser erzeugte Ableitungsbaum der Rechtsableitung (Rechtsableitung ersichtlich dadurch, dass sich rechts alle Änderungen passieren, und die linke Seite unberührt bleibt) ergibt sich aus den ersten beiden Spalten, von unten nach oben gelesen.
\begin{lstlisting}[language=C]
%{
#include "calc.tab.h"
%}

integer [0-9]+
real {integer}("."{integer})?([eE][+-]?{integer})?

%%

{real}			{yylval.number = atof(yytext); return NUM;}
[ \t]+			;
\n					{return NL;}
...

// Mit diesem Code kann mit Bison ein C-Programm erzeugt werden, dass diesen Parser implementiert
\end{lstlisting}


\subsection{OL-Systeme}
Zur Darstellung benötigen wir Turtle-Grafik:\\
ABB 136\\
Befehle:\\
$forward (l)$ \\
$left (\alpha)$ bzw. $right(\alpha)$

Im Unterschied zu einer kontextfreien Grammatik wird in einem OL-System in jedem Ableitungsschritt jede Variable ersetzt. Jede Variable wird dabei durch die gleiche Regel ersetzt. Anstelle eines Startsymbols gibt es eine initiale Satzform.
\subparagraph{Bsp.:} \parskp
Variable: $F$\\
Symbole: $+,-$\\
Regel: $F\to F+F--F+F$\\
$F\Rightarrow F+F --F + F \\
\Rightarrow \underbrace{F + F -- F + F}_{1.\,F} + \underbrace{F + F -- F + F}_{2.\,F} -- \underbrace{F + F -- F + F}_{usw.} + \underbrace{F + F -- F + F}$\\
Graphische Darstellung:\\
$F= forward$\\
$+=left(60^\circ)$\\
$-=right(60^\circ)$\\
ABB 137
\section{Die Chomsky Hierarchie}
Für Grammatiken:\\
\begin{tabular}{L{0.3} L{0.5} L{0.2}}
Typ & in aller Regeln $u\to r$ gilt & Beispiele\\
\hline
0 (rekursiv aufzählbar) & $u,r$ beliebig & $ab\to c$\\
1 (kontext sensitiv) & $u=\alpha X \beta, \; v = \alpha \gamma \beta$ mit $\alpha, \beta \in (V\cup \Sigma)^*, \; x \in V, \; \gamma \in (V\cup \Sigma)^+$ & $aAb \to aBb$\\
2 (kontextfrei) & $u \in V, \; v\in (V \cup \Sigma)^*$ & $A \to aBb$\\
3 (regulär) & $u\in V, \; v \in \Sigma \cup \{\varepsilon\} \cup \Sigma V$ & $A \to a$, $A\to aB$
\end{tabular}\\
weitere Beispiele:
\begin{enumerate}[start=0]
\item $(a+b) \cdot c \to a\cdot c + b \cdot c$
\item Artikel$_m$ Nomen $\to$ Artikel$_m$ Kater\\
Artikel$_f$ Nomen $\to$ Artikel$_f$ Katze
\item - 
\item (ist linear im rechten Teilbaum entartet)
\end{enumerate}
Chomsky-Hierarchie mit Beispielsprachen:\\
ABB 138\\
Eine Sprache hat mindestens die gleiche Klasse wie seine Grammatik, kann aber auch eine höhere haben. Für eine reguläre Sprache muss ein regulärer Ausdruck konstruierbar sein.
\chapter{Berechenbarkeit und Komplexität}
Frage: Was können Computer berechnen, was können Computer effizient berechnen?\\
ABB 139 (Turing Maschine)
\section{Entscheidbarkeit}
Entscheidungsproblem: Gegeben eine Sprache $L$ und ein Wort $w\in \Sigma^*$. Gehört $w$ zu $L$ ($w\in L$)?

Wir kennen bereits entscheidbare Probleme:
\begin{itemize}
\item Wenn $L$ als regulärer Ausdruck gegeben ist, dann ist $w\in L$ entscheidbar durch folgendes Verfahren: 
\begin{itemize}
\item Regulären Ausdruck in DFA umwandeln.
\item DFA die Eingabe $w$ übergeben.
\item Wenn DFA einen Endzustand erreicht gilt $w\in L$, sonst $w\not \in L$.
\end{itemize}
\item $L=\emptyset$ ist entscheidbar, durch ein Entscheidungsverfahren, das immer „falsch“ liefert.
\item $L=\Sigma^*$ ist entscheidbar, durch ein Entscheidungsverfahren, das immer „richtig“ liefert.
\item Wenn $L$ als kontextfreie Grammatik gegeben ist, ist $L$ entscheidbar über:
\begin{itemize}
\item Umformung in CNF
\item Anwendung des CYK-Algorithmus
\end{itemize}
\end{itemize}

\paragraph{Def.:} Eine Sprache $L$ heißt \emph{entscheidbar}, wenn es ein Programm $P_L$ (Entscheidungsverfahren) gibt, wenn gilt
\begin{itemize}
\item für $w\in L$ liefert $P_L$ die Ausgabe $true$
\item für $w \not \in L$ liefert $P_L$ die Ausgabe $false$.
\end{itemize}

\subparagraph{Bsp.:}
\begin{itemize}
\item Wenn $L$ eine kontextfreie Sprache, ist, dann ist $L$ entscheidbar durch den CYK-Algorithmus.
\item Die Sprache $L= \{ (M,w) \;|\; M \text{ ist ein DFA mit }w\in L(M)\}$ ist entscheidbar durch ein Programm des aus der Eingabe $(M,w)$ eine Repräsentation des DFA erstellt und damit $M$ für die Eingabe $w$ simuliert (vgl. Darstellung eines Interpreters in Zusammenhang mit der erweiterten Überführungsfunktion $\tilde{\delta}$).
\item Die Sprache $\{ M \;|\; M \text{ ist ein DFA mit }L(M)=\Sigma^*\}$ ist entscheidbar (siehe Übung).
\item Allgemein: $\{(P,w)\;|\;P \text{ hält für }w\}$. Im Beispiel: Collatz-Problem.
\end{itemize}
\section{Halteproblem}
Das Halteproblem ist die Sprache $H=\{ (P,w) \;|\; \text{Das Programm $P$ hält für die Eingabe }w\}$. Die Frage, ob ein Programm $P$ für eine gegebene Eingabe $w$ hält, ist damit gleichwertig zur Frage $(P,w)\in H$. Wir beweisen zunächst die Unentscheidbarkeit eines Spezialfalls.
\paragraph{Satz:} Das spezielle Halteproblem $K=\{P\;|\;\text{das Programm $P$ hält für die Eingabe }P\}$ ist unentscheidbar.
\subparagraph{Bsp.:}
\begin{itemize}
\item für ein Programm $P \in K$:\begin{lstlisting}[language=C]
void P( Input w ) {
	return;
}
\end{lstlisting}
\item für ein Programm $P \not \in K$:
\begin{lstlisting}[language=C]
void P( Input w ) {
	while(true);
}
\end{lstlisting}
\end{itemize}
Beweis (Widerspruch):\\
Angenommen, $K$ sei entscheidbar durch ein Entscheidungsverfahren $P_K$. Daraus konstruieren wir ein Programm $P_K^*$, das $P_K$ als Unterprogramm verwendet und das
\begin{itemize}
\item in eine Endlosschleife übergeht, wenn $P_K$ $true$ liefert
\item hält, wenn $P_K$ $false$ liefert.
\end{itemize}
ABB 148\\
Nach Konstruktion gilt dann:
\begin{itemize}
\item Für die Eingabe $P$ hält $P_K^*$ genau dann, wenn $P_k$ false enthält.
\end{itemize}
Da nach Annahme $P_K$ ein Entscheidungsverfahren für $K$ ist, bedeutet das:
\begin{itemize}
\item Für die Eingabe $P$ hält $P_K^*$ genau dann, wenn $P$ für die Eingabe $P$ nicht hält.
\end{itemize}
Für $P=P_K^*$ folgt:
\begin{itemize}
\item Für die Eingabe $P_K^*$ hält $P_K^*$ genau dann, wenn $P_K^*$ für die Eingabe $P_K^*$ nicht hält.
\end{itemize}
Widerspruch!\medskip\\
Folgerung: Das Halteproblem $H$ ist unentscheidbar.\\
Beweis: Angenommen, $H$ wäre entscheidbar durch $P_H$. Dann können wir folgendes Entscheidungsverfahren für $K$ konstruieren:
\begin{lstlisting}[language=C]
bool P_K ( Programm P ) {
	return P_H(P,P);
}
\end{lstlisting}
Widerspruch!

\subsection{Weitere unentscheidbare Probleme}
Um die Unentscheidbarkeit weiterer Probleme zu zeigen, verwenden wir einen Beweis durch Widerspruch nach folgender Bauart:
\begin{itemize}
\item Um die Unentscheidbarkeit einer Formalen Sprache $B$ zu zeigen, verwenden wir eine unentscheidbare Sprache $A$.
\item Wir nehmen an, dass die Sprache $B$ entscheidbar ist. Folglich gibt es ein Entscheidungsverfahren für $B$.
\item Wir zeigen, dass sich damit ein Entscheidungsverfahren für A konstruieren lässt.\\
Widerspruch!
\end{itemize}

Anwendung:
\paragraph{Satz:} Das Halteproblem $H$ ist nicht entscheidbar.\\
$H=\{(P,w)\;|\; P \text{ hält für }w \}$\\
$K=\{P\;|\;P \text{ hält für } P\}$
\begin{proof}
Angenommen $H$ ist entscheidbar. \\
Dann gibt es ein Entscheidungsverfahren $P_H$ für $H$. \\
Dann können wir folgendes Programm konstruieren:
\begin{lstlisting}[language=Java]
boolean $P_K$ (Program P) {
	return $P_H$(P,P);
}
\end{lstlisting}
Dann gilt: $\underline{P\in K} \Leftrightarrow P \text{ hält für } P \Leftrightarrow (P,P) \in H \Leftrightarrow \underline{P_H(P,P) \text{ ist true}}$\\
Widerspruch, da $K$ unentscheidbar ist.
\end{proof}
\paragraph{Satz:} $H_\varepsilon = \{P \;|\; P \text{ hält für die Eingabe }\varepsilon\}$
\begin{proof}
Angenommen, $H_\varepsilon$ ist entscheidbar.\\
Dann gibt es Entscheidungsverfahren $P_{H_\varepsilon}$ für $H_\varepsilon$.\\
Damit können wir folgendes Programm konstruieren:
\begin{lstlisting}[language=Java]
boolean $P_H$ (Program P, Input w){
	void F() {
		P(w);
	}
	return $P_{H_\varepsilon}$(F);
}
\end{lstlisting}
Dann gilt: $(P,w) \in H \Leftrightarrow P \text{ hält für }w \Leftrightarrow F \text{ hält für }\varepsilon \Leftrightarrow F \in H_\varepsilon$\\
Widerspruch, da $H$ unentscheidbar ist.
\end{proof}
\paragraph{Satz:} $H^*=\{P \;|\; P \text{ hält für jede Eingabe}\}$ ist nicht entscheidbar.
\begin{proof}
Angenommen, $H^*$ ist entscheidbar durch ein Entscheidungsverfahren $P_{H^*}$.\\
Dann können wir folgendes Programm konstruieren:
\begin{lstlisting}[language=Java]
boolean $P_{H_\varepsilon}$(Program P) {
	void F(Input w){
		P($\varepsilon$);
	}
	return $P_{H^*}$(F);
}
\end{lstlisting}
Dann gilt: $P\in H_\varepsilon \Leftarrow P \text{ hält für die Eingabe }\varepsilon\Leftrightarrow F \text{ hält für jede Eingabe }w \Leftrightarrow F \in H^*$\\
Widerspruch, da $H_\varepsilon$ nicht entscheidbar.
\end{proof}

\paragraph{Satz:} $\ddot{A}=\{(P_1, P_2) \;|\; P_1, P_2 \text{ berechnen die gleichen Funktionen}\}$ ist nicht entscheidbar.
\begin{proof}
Angenommen $\ddot{A}$ ist entscheidbar durch $P_{\ddot{A}}$.\\
Dann konstruieren wir das Programm:
\begin{lstlisting}[language=Java]
boolean $P_{H^*}$(Program P){
	int F(Input w){
		return 0;
	}
	int G(Input w){
		P(w);	// hält P(w) immer? Nur dann ist G 0.
		return 0;
	}
	return $P_{\ddot{A}}$(F,G);
}
\end{lstlisting}
Dann gilt: $P\in H^* \Leftrightarrow P \text{ hält für jede Eingabe }w \Leftrightarrow G \text{ berechnet }w\mapsto 0\\
\Leftrightarrow F,G \text{ berechnen die gleiche Funktion}\Leftrightarrow (F,g)\in \ddot{A}$ (genau dann, wenn ich $H^*$ entscheiden kann, kann ich $\ddot{A}$ entscheiden)\\
Widerspruch, da $H^*$ unentscheidbar. 
\end{proof}

\subsubsection{Unentscheidbarkeit der Programmverifikation}
ABB 170\\
Aus der Unentscheidbarkeit des Halteproblems folgt:\\
$\{ (P,S) \; | \; $Das Programm $P$ erfüllt die Spezifikation $S\}$ ist unentscheidbar.\\
Auch unentscheidbar:
\begin{itemize}
\item $\{P\;|\;P$ verursacht keine Division durch $0\}$
\item $\{P\;|\;P$ verursacht keine Array-out-Bound-Fehler$\}$
\item $\{P\;|\;P$ dereferenziert keine Nullpointer$\}$
\end{itemize}
Möglicher Ausweg:\\
Verifizierer liefert ja, nein \emph{oder} unbekannt.

\chapter{Komplexität}
Frage: Gibt es Probleme, die zwar lösbar (entscheidbar) sind, aber dazu einen sehr großen Rechenaufwand erfordern?\medskip\\
Kostenmaße:
\begin{itemize}
\item Uniforme Kostenmaß:\\
Alle Operationen erfordern konstanten Aufwand (LZ in $O(1)$)
\item Logarithmisches Kostenmaß:\\
Alle Operationen erfordern logarithmischen Aufwand (LZ in $O(\log n)$)
\end{itemize}
Im folgenden verwenden wir das uniforme Kostenmaß, wo angemessen.

\section{Die Klasse P}
\paragraph{Def.:} $P=\bigcup_{k>0} \{ L\;|\; L$ ist entscheidbar durch ein Programm LZ in $O(n^k)\}$
\subparagraph{Bsp.:} Folgende Sprachen bzw. Probleme liegen in $P$:
\begin{itemize}
\item $\emptyset$, $\Sigma^*$ (LZ $O(1)$)
\item $\{a^n\;| \; n\geq 0\}$ (muss maximal $n$ Zeichen überprüfen: LZ $O(n)$)
\item Jede kontextfreie Sprache $L$, da $L$ duch den CYK-Algorithmus in Zeit $O(n^3)$ entschieden werden kann.
\end{itemize}
\subparagraph{Bsp.:} Das Problem $PFAD=\{(G,n_1,n_2)\;|\; G$ ist ein Graph, in dem es einen Pfad von $n_1$ nach $n_2$ gibt $\}$ liegt in $P$.\\
Veranschaulichung:\\
ABB 171\\
Der Graph $G$ sei dabei als Adjazenzliste gegeben.
\begin{proof}
Da die Adjazenzliste von $G=(V,E)$ mindestens $|V|+|E|$ Elemente enthält, gilt für die Länge $n$ der Eingabe: $n\geq |V|+|E|$.\\
Ein Entscheidungsverfahren für $PFAD$ ist eine in $n_1$ gestartete Breitensuche nach $n_2$, die die LZ $O(|V|+|E|)$ besitzt. Folglich ist die LZ des Entscheidungsverfahrens $\leq c \cdot (|V|+|E|) \leq c \cdot n \in O(n)$. 
\end{proof}
Das gleiche Ergebnis erhalten wir, wenn die LZ nur in $|V|$ gemessen wird: Denn es gilt $n \geq |V|$. Für die LZ der Breitensuche gilt: LZ$\in O(|V|+|E|)\subseteq O (|V|+|V|^2)=O(|V|^2)\subseteq O(n^2)$.  Auch damit folgt $PFAD \in P$.\medskip\\
Die Klasse $P$ wird betrachtet als Klasse der effizient lösbaren Probleme.

\section{Die Klasse NP}
Die Klasse $NP$ (nicht deterministisch polynomiell) enthält alle Probleme, die in polynomieller Zeit verifizierbar sind.
\subparagraph{Bsp.:} $PFAD \in NP$, denn mit Hilfe eines Zertifikates lässt sich prüfen, dass es in $G$ einen Pfad von $n_1$ nach $n_2$ gibt und die LZ dafür ist polynomiell:\\
Das Zertifikat ist der Pfad selbst, die LZ liegt in $O(|V|^2)$.
\subparagraph{Bsp.:} $SAT=\{F\;|\;F$ ist eine erfüllbare Formel der Aussagenlogik$\}$. Zum Beispiel gilt $x\vee y \in SAT$, $x\wedge \neg x \not \in SAT$.\\
$SAT$ ist entscheidbar durch die Konstruktion einer Wahrheitstabelle. Wenn $F$ $n$ Variablen enthält, benötigt dies die LZ $O(2^n)$ (liegt also nicht in $P$).\\
$SAT$ lässt sich jedoch effizient (d.h. in polynomieller Zeit) verifizieren, wenn als Zertifikat eine erfüllende Belegung für $F$ gegeben ist. Die LZ für die Verifikation liegt dann in $O(|F|)$. Deshalb gilt $SAT \in NP$.

\newpage
\printbibliography
\end{document}
